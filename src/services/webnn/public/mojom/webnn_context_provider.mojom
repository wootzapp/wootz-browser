// Copyright 2023 The Chromium Authors
// Use of this source code is governed by a BSD-style license that can be
// found in the LICENSE file.

module webnn.mojom;

import "components/ml/webnn/features.mojom";
import "services/webnn/public/mojom/webnn_buffer.mojom";
import "services/webnn/public/mojom/webnn_error.mojom";
import "services/webnn/public/mojom/webnn_graph.mojom";
import "mojo/public/mojom/base/unguessable_token.mojom";

// Represents options of creating `WebNNContext` interface.
struct CreateContextOptions {
  enum Device {
    // Provides the broadest compatibility and usability across all client
    // devices with varying degrees of performance.
    kCpu,
    // Provides the broadest range of achievable performance across graphics
    // hardware platforms from consumer devices to professional workstations.
    kGpu,
    // NPU (neural processing unit) is a class of specialized hardware
    // accelerator designed to accelerate artificial intelligence and machine
    // learning applications. Unlike more general-purpose devices such as the
    // GPU and CPU, an NPU supports a limited finite set of operations and may
    // not have programmability support. The fallback behavior is being
    // discussed by WG at:
    // https://github.com/webmachinelearning/webnn/issues/623.
    // This Enum value is introduced for testing purpose that will inform
    // the WG.
    kNpu,
  };

  enum PowerPreference {
    // Let the user agent select the most suitable behavior.
    kDefault,
    // Prioritizes execution speed over power consumption.
    kHighPerformance,
    // Prioritizes power consumption over other considerations such as execution
    // speed.
    kLowPower,
  };

  // Indicates the kind of device used for the context.
  Device device;
  // The power preference for power consumption.
  PowerPreference power_preference;

  // Hint for the number of threads to use for inference, with zero indicating a
  // preference to defer this decision to the backend. A backend may also choose
  // to ignore this value entirely.
  //
  // TODO(crbug.com/338162119): This is not the final expected API shape. Spec
  // discussion on how this information should be communicated is happening
  // here: https://github.com/webmachinelearning/webnn/issues/436
  uint8 thread_count_hint;
};

// Represents the return value of `WebNNContext::CreateGraph()`. Let it be
// `graph_remote` if the graph was successfully created and `error` otherwise.
union CreateGraphResult {
  pending_associated_remote<WebNNGraph>? graph_remote;
  Error error;
};

// Represents the return value of `WebNNContextProvider::CreateWebNNContext()`.
// Let it be `context_remote` if WebNNContext was successfully created and
// `error` otherwise.
union CreateContextResult {
  pending_remote<WebNNContext>? context_remote;
  Error error;
};

// Represents the `MLContext` object in the WebIDL definition that is a global
// state of neural network compute workload and execution processes. This
// interface runs in the GPU process and is called from the renderer process.
[RuntimeFeature=webnn.mojom.features.kWebMachineLearningNeuralNetwork]
interface WebNNContext {
  // Called by the renderer process to create `WebNNGraph` message pipe for
  // executing computational graph, the WebNN graph will be validated and
  // compiled. Initializes the compiled graph for optimal performance of the
  // subsequent graph executions if the initialization is a necessary step.
  CreateGraph(GraphInfo graph_info) => (CreateGraphResult result);

  // Called by the renderer process to create `WebNNBuffer` message pipe for
  // creating platform specific buffers, the WebNN buffer will be validated and
  // created. This method guarantees memory allocation on the device.
  // The receiver represents the connection to the MLBuffer and the
  // buffer_handle is a generated token used as a handle to identify the buffer
  // from the renderer.
  CreateBuffer(pending_associated_receiver<WebNNBuffer> receiver,
      BufferInfo buffer_info, mojo_base.mojom.UnguessableToken buffer_handle);
};

// This interface runs in the GPU process and is called from the renderer
// process to create `WebNNContext` interface. The renderer requests this
// interface from the frame's BrowserInterfaceBroker, which requests it
// from the GpuService via the GpuClient.
[RuntimeFeature=webnn.mojom.features.kWebMachineLearningNeuralNetwork]
interface WebNNContextProvider {
  // Called by the renderer process to create a message pipe for `MLContext`,
  // the `CreateContextResult` will be `Error` with error messages if the
  // configuration of options isn't supported.
  CreateWebNNContext(CreateContextOptions options)
      => (CreateContextResult result);
};
